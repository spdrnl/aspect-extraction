{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised aspect extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8HhYMmYMinWoFPVA8yMgd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spdrnl/aspect-extraction/blob/master/Unsupervised_aspect_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcLPx-B8gYZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "49a94e8a-1583-40e2-c5b0-69f361d79dcd"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend as K\n",
        "import gensim\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "from google.colab import drive\n",
        "from os import path\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z157Fan3ZnA",
        "colab_type": "text"
      },
      "source": [
        "# General settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WktSb0FgbLBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf8a92c2-2a73-485e-8bfd-4365cc0e7aed"
      },
      "source": [
        "# Mount Google drive to save generated data and models\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# The location of the downloaded raw data\n",
        "file_name = \"/tmp/reviews_Beauty.json.gz\"\n",
        "\n",
        "# The location of the cleaned data\n",
        "corpus_file = \"/content/drive/My Drive/corpus.txt\"\n",
        "\n",
        "# The size of the word vectors\n",
        "vector_size = 200\n",
        "\n",
        "# The location of the gensim model\n",
        "model_file = '/content/drive/My Drive/w2v_model.gensim'\n",
        "\n",
        "# The maximum allowed length of a sentence\n",
        "max_length = 35\n",
        "\n",
        "# The assumed number of aspects, this is a hyperparameter\n",
        "n_aspects = 14"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNSYf6g6XmP7",
        "colab_type": "text"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo3k-7O_gb0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "2dbbbdac-6b07-472b-d997-d75bf057ec60"
      },
      "source": [
        "! wget -P /tmp http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-04 14:23:13--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 352748278 (336M) [application/x-gzip]\n",
            "Saving to: ‘/tmp/reviews_Beauty.json.gz.1’\n",
            "\n",
            "reviews_Beauty.json 100%[===================>] 336.41M  7.11MB/s    in 31s     \n",
            "\n",
            "2020-07-04 14:23:44 (10.9 MB/s) - ‘/tmp/reviews_Beauty.json.gz.1’ saved [352748278/352748278]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Irjm_k331C",
        "colab_type": "text"
      },
      "source": [
        "## Show a sample of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdW01AuwyKwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "d031fc0c-0def-469b-ae57-8a76ffc091be"
      },
      "source": [
        "with gzip.open(file_name) as f:\n",
        "  for id, line in enumerate(f):\n",
        "    print(line)\n",
        "    if id >= 10:\n",
        "      break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2dc8c1025cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/reviews_Beauty.json.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6eVpZtvXsjZ",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxddMbfohq2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 0\n",
        "max_observed_length = 0\n",
        "\n",
        "# Sentences are split on .?!\n",
        "split_pattern = re.compile('[\\.\\?!]')\n",
        "\n",
        "# Remove all non alphanumerical values, except spaces\n",
        "pattern = re.compile('[^a-zA-Z0-9\\s]+')\n",
        "\n",
        "# Consolidate all sequential white space to a single space\n",
        "white_space_pattern = re.compile('\\s+')\n",
        "\n",
        "stop_words = set([\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \n",
        "                  \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \n",
        "                  \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \n",
        "                  \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \n",
        "                  \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
        "                  \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \n",
        "                  \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \n",
        "                  \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n",
        "                  \"through\", \"during\", \"before\", \"after\", \"to\", \"from\", \n",
        "                  \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \n",
        "                  \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \n",
        "                  \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\",\n",
        "                  \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \n",
        "                  \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
        "\n",
        "# Clean the data if not already done so\n",
        "# The cleaned data will be output to disk\n",
        "if not path.exists(corpus_file):\n",
        "  with open(corpus_file, mode='w') as out:\n",
        "    with gzip.open(file_name) as f:\n",
        "        for line in f:\n",
        "          raw_review_text = json.loads(line)['reviewText']\n",
        "          lower_review_text = raw_review_text.lower()\n",
        "          for review_line in lower_review_text.splitlines():\n",
        "            for sentence in split_pattern.split(review_line):\n",
        "              sentence = pattern.sub('', sentence)\n",
        "              sentence = white_space_pattern.sub(' ', sentence)\n",
        "              words = [word for word in sentence.split() if not word in stop_words]\n",
        "              max_observed_length = len(words) if len(words) > max_observed_length else max_observed_length\n",
        "              sentence = \" \".join(words[:max_length])\n",
        "              if len(sentence) > 5: \n",
        "                out.write(\"{}\\n\".format(sentence))\n",
        "                n += 1\n",
        "\n",
        "  print(\"Created {} sentences with a maximum length of {}\".format(n, max_length))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jay9cWLo4Z3K",
        "colab_type": "text"
      },
      "source": [
        "## Sample the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md4kLIJMjQY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "452d7054-fb2a-450d-d738-529e319a288d"
      },
      "source": [
        "! head '$corpus_file'"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "love moisturizer would recommend someone dry skin fine lines wrinkles\n",
            "using brand day night serum\n",
            "received product deadline\n",
            "tested baby kabuki quality material best\n",
            "packaging cute\n",
            "fibers not smell soft\n",
            "love set\n",
            "great buy price\n",
            "dont wear makeup time love feels\n",
            "nice moisturizer natural ingredients no parabens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P606CW_X7IX",
        "colab_type": "text"
      },
      "source": [
        "# Create word vectors using Gensim from sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9vuvgznlsRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeautyCorpus(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = out_file\n",
        "        for line in open(corpus_path):\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31yjBrbFrW5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "969b45c2-87ef-4c2f-ad3c-b03c0df2a7de"
      },
      "source": [
        "from os import path\n",
        "\n",
        "# Only run gensim if there is no previous model file\n",
        "if not path.exists(model_file):\n",
        "  model = gensim.models.Word2Vec(sentences=BeautyCorpus(), min_count=10, size=vector_size, workers=1, window=10, negative=5)\n",
        "  model.save(model_file)\n",
        "else:\n",
        "  model = gensim.models.Word2Vec.load(model_file)\n",
        "\n",
        "print(\"There are {} words in the word2ve model\".format(len(model.wv.index2word)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There are 46047 words in the word2ve model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4D3p9tdcBhr",
        "colab_type": "text"
      },
      "source": [
        "# Format input data for ABAE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hf2UHP11aMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note <PAD> is added to the vocabulary to allow for padding in the ABAE model\n",
        "index2word = ['<PAD>'] + model.wv.index2word\n",
        "word2index = {word: idx for idx, word in enumerate(index2word)}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLVNdqce2zwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_sentences = []\n",
        "with open(corpus_file) as corpus:\n",
        "  for line_idx, line in enumerate(corpus):\n",
        "    words = line.split()[:max_length]\n",
        "    encoded_words = [word2index.get(word, -1) for word in words if word2index.get(word, -1) != -1]\n",
        "    if len(encoded_words) > 0:\n",
        "      encoded_words = encoded_words + [0] * max_length\n",
        "      encoded_words = encoded_words[:max_length]\n",
        "      encoded_sentences.append(encoded_words)\n",
        "encoded_sentences = np.array(encoded_sentences)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GEv0gXt7u_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = np.zeros([len(index2word), vector_size])\n",
        "for idx, word in enumerate(index2word):\n",
        "  if idx == 0: continue\n",
        "  else: embeddings[idx] = model.wv[word]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-RxUTL5X1Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "sample_size = 100000\n",
        "sample_words = encoded_sentences[:sample_size].flatten().astype(np.int32)\n",
        "sample_words = sample_words[sample_words > 4]\n",
        "embedded_words = embeddings[sample_words, :]\n",
        "kmeans = KMeans(n_clusters=14, random_state=0).fit(embedded_words)\n",
        "T_init = kmeans.cluster_centers_\n",
        "sample_words = None\n",
        "embedded_words = None\n",
        "T_init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E_pK3bMYLSh",
        "colab_type": "text"
      },
      "source": [
        "# Create  Attention-based Aspect  Extraction (ABAE) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hydP8I-IokZc",
        "colab_type": "text"
      },
      "source": [
        "## Create the ABAE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBeByK_NLECx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionBasedEncoder(keras.layers.Layer):\n",
        "  def __init__(self, embeddings, max_sentence_length):\n",
        "    super(AttentionBasedEncoder, self).__init__()\n",
        "    self.vocab_size = embeddings.shape[0]\n",
        "    self.vector_size = embeddings.shape[1]\n",
        "    self.max_sentence_length = max_sentence_length\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(self.vocab_size, \n",
        "                                            self.vector_size, \n",
        "                                            embeddings_initializer=keras.initializers.Constant(embeddings),\n",
        "                                            trainable=False,\n",
        "                                            mask_zero=True)\n",
        "    \n",
        "    self.M = tf.Variable(tf.random.uniform([self.vector_size,self.vector_size], minval=-.00001, maxval=.00001), name='M')\n",
        "\n",
        "  def get_words_per_sentence(self, x):\n",
        "    # x is batch_size, max_sentence_length\n",
        "    non_padding_mask = tf.cast(tf.math.greater(x, 0), tf.float32)\n",
        "    words_per_sentence = tf.reduce_sum(non_padding_mask, axis=-1)\n",
        "    return words_per_sentence\n",
        "\n",
        "  def get_y_s(self, x, e):\n",
        "    # y_s is batch_size, vector_size\n",
        "    words_per_sentence = self.get_words_per_sentence(x)\n",
        "    y_s = tf.reduce_sum(e, axis=1) / tf.expand_dims(words_per_sentence, axis=-1)\n",
        "    return y_s\n",
        "\n",
        "  def get_d_i(self, e, y_s):\n",
        "    # d_i is batch_size, max_sentence_length\n",
        "    tmp = tf.expand_dims(tf.matmul(y_s, self.M), axis=-1)\n",
        "    d_i = tf.matmul(e, tmp)  \n",
        "    return d_i\n",
        "\n",
        "  def get_a_i(self, d_i):\n",
        "    # a_i is batch_size, max_sentence_length \n",
        "    exp_d = tf.exp(d_i)\n",
        "    tmp = tf.expand_dims(tf.reduce_sum(exp_d, axis=1), axis=-1)\n",
        "    a_i = tf.divide(exp_d, tmp)\n",
        "    return a_i\n",
        "\n",
        "  def get_z_s(self, a, e):\n",
        "    # z_s is batch_size, vector_size\n",
        "    tmp = tf.multiply(e, a)\n",
        "    z_s = tf.reduce_sum(e * tmp, axis=1)\n",
        "    return z_s\n",
        "\n",
        "  def call(self, x): \n",
        "\n",
        "    # e is batch_size, max_sentence_length, vector_size\n",
        "    e = self.embedding(x) \n",
        "    \n",
        "    # y_s is batch_size, vector_size\n",
        "    y_s = self.get_y_s(x, e)\n",
        "    \n",
        "    # d_i is batch_size, max_sentence_length\n",
        "    d_i = self.get_d_i(e, y_s)\n",
        "\n",
        "    # a_i is batch_size, max_sentence_length \n",
        "    a_i = self.get_a_i(d_i)\n",
        "\n",
        "    # z_s is batch_size, vector_size\n",
        "    z_s = self.get_z_s(a_i, e)\n",
        "\n",
        "    return z_s"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oVyBf4rQp2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  # Define the input layers\n",
        "  pos_input = keras.layers.Input(shape=[max_length], name='pos_input')\n",
        "  neg_input = keras.layers.Input(shape=[max_length], name='neg_input')\n",
        "\n",
        "  # Apply the attention based encoder to the input layers\n",
        "  attention_based_encoder = AttentionBasedEncoder(embeddings, max_sentence_length=max_length)\n",
        "  pos_zs = attention_based_encoder(pos_input)\n",
        "  neg_zs = attention_based_encoder(neg_input)\n",
        "\n",
        "  # Calculate the rs value for the positive samples\n",
        "  pos_pt = keras.layers.Dense(n_aspects, activation='softmax', name='pt')(pos_zs)\n",
        "  pos_rs = keras.layers.Dense(vector_size, use_bias=False, kernel_initializer=keras.initializers.Constant(T_init), name='rs')(pos_pt)\n",
        "\n",
        "  # Output the pos_rs, pos_zs, neg_zs values necessary for the triplet loss\n",
        "  nn = tf.keras.Model(inputs=[pos_input, neg_input], outputs=[pos_rs, pos_zs, neg_zs])\n",
        "\n",
        "  return nn"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWu_EDu6ohVB",
        "colab_type": "text"
      },
      "source": [
        "## Train the ABAE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2my8Na3dg-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDatasetGenerator:\n",
        "  \"\"\" A dataset can be created using this generator. The generator outputs \n",
        "  tuples of positive and negative samples. To correct for the number of\n",
        "  negative samples, a negative factor is used to repeat the training data.\n",
        "  This setup replicates the original loss function, trading of more compute time\n",
        "  for simpler code.\"\"\" \n",
        "\n",
        "  def __init__(self, encoded_sentences, negative_factor=20):\n",
        "    self.n = encoded_sentences.shape[0]\n",
        "    self.encoded_sentences = encoded_sentences\n",
        "    self.pos_sentence_idxs = np.repeat(np.arange(n), repeats=negative_factor)\n",
        "    self.neg_sentence_idxs = np.repeat(np.arange(n), repeats=negative_factor)\n",
        "    np.random.shuffle(self.pos_sentence_idxs)\n",
        "    np.random.shuffle(self.neg_sentence_idxs)\n",
        "\n",
        "  def __call__(self):\n",
        "    for x, y in zip(self.pos_sentence_idxs, self.neg_sentence_idxs):\n",
        "      yield (encoded_sentences[x,:],encoded_sentences[y, :])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t10RV1Q2gAiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3aa8fb7-8c67-4c1c-e997-1e46c0b4aec6"
      },
      "source": [
        "! rm -rf logs\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "import datetime\n",
        "\n",
        "#@tf.function\n",
        "def train(dataset, model, objective_fn, regularization_fn, optimizer, regularizer, num_training_samples, batch_size, train_summary_writer, train_los):\n",
        "\n",
        "  for batch, (pos_samples, neg_samples) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      [rs_pos, zs_pos, zs_neg] = model([pos_samples, neg_samples], training=True)\n",
        "      J = objective_fn(rs_pos, zs_pos, zs_neg)\n",
        "      U = regularization_fn(model)\n",
        "      loss = J + regularizer * U\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.scalar('loss', train_loss.result(), step=batch)\n",
        "\n",
        "#@tf.function \n",
        "def regularization_fn(model):\n",
        "    T = tf.squeeze(model.get_layer(\"rs\").weights)\n",
        "    TT = tf.matmul(T, tf.transpose(T))\n",
        "    U = tf.reduce_sum(tf.square(TT - tf.identity(TT)))\n",
        "    return U\n",
        "\n",
        "#@tf.function\n",
        "def objective_fn(rs_pos, zs_pos, zs_neg):\n",
        "  o = 1 - tf.reduce_sum(rs_pos * zs_pos, axis=-1) + tf.reduce_sum(rs_pos * zs_neg, axis=-1)\n",
        "  mask = tf.cast(tf.math.greater(o, 0), tf.float32)\n",
        "  objective = tf.reduce_sum(o * mask, axis=0)\n",
        "  return objective\n",
        "\n",
        "batch_size = 64\n",
        "dataset = tf.data.Dataset.from_generator(MyDatasetGenerator(encoded_sentences, 5), output_types=(tf.int32, tf.int32)).repeat(15).batch(batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.0000001)\n",
        "\n",
        "regularizer = 1.0\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "train(dataset, get_model(), objective_fn, regularization_fn, optimizer, regularizer, n, batch_size, train_summary_writer, train_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1726), started 0:22:28 ago. (Use '!kill 1726' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fe29dbba82c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-fe29dbba82c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, objective_fn, regularization_fn, optimizer, regularizer, num_training_samples, batch_size, train_summary_writer, train_los)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;34m[\u001b[0m\u001b[0mrs_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs_neg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularization_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av7qFW9HYWUu",
        "colab_type": "text"
      },
      "source": [
        "# Testing section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anF_mgOSoyqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4ceeb3d-bced-44f0-89f8-ecb857b54188"
      },
      "source": [
        "!kill 1726"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: (1726) - No such process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HShS6S8U1Ej1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train(dataset, model, loss_fn, optimizer):\n",
        "  for x, y in dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      # training=True is only needed if there are layers with different\n",
        "      # behavior during training versus inference (e.g. Dropout).\n",
        "      prediction = model(x, training=True)\n",
        "      loss = loss_fn(y, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def loss_fn(y, prediction):\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(y, prediction)))\n",
        "\n",
        "a = tf.Variable(10.0, dtype=tf.float32)\n",
        "b = tf.Variable(5.0, dtype=tf.float32)\n",
        "input = tf.keras.layers.Input([1])\n",
        "output = tf.add(a, tf.multiply(input, b))\n",
        "model = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "# dx = np.arange(-100,100, dtype=np.float32)\n",
        "# dy = (10.0 + 3.0 * dx + np.random.rand(200)).astype(np.float32)\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((dx.reshape([-1,1]), dy)).batch(25)\n",
        "dataset = tf.data.Dataset.from_generator(DatasetGenerator(-100,100), output_types=(tf.float32, tf.float32)).batch(25)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "train(dataset, model, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCO5ztzihmmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8f9e1fa-c9d5-4b0c-bb78-1971c3b8bf64"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zJurkCynH4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb906bfb-32da-47f4-bcf7-16aacceef5ac"
      },
      "source": [
        "model.predict([5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[35.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BziIbLanPo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(dataset.as_numpy_iterator())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLWLNYrDoKCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDatasetGenerator:\n",
        "\n",
        "  def __init__(self, low, high):\n",
        "    data = np.arange(low, high, dtype=np.float32)\n",
        "    self.X = data.reshape([-1,1])\n",
        "    self.y = (10.0 + 3.0 * data + np.random.rand(200)).astype(np.float32)\n",
        "\n",
        "  def __call__(self):\n",
        "    for x, y in zip(self.X, self.y):\n",
        "      yield (x,y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv9UHlcVMPnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}